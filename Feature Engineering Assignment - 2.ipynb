{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad48a8e-8016-4306-8834-5783f115c5a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020712a2-02f2-48ff-93a6-c836d467b05c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform features in a dataset so that they all fall within a specified range, usually between 0 and 1. This method is particularly useful when the features have different scales, and you want to ensure that they are on a comparable scale for machine learning algorithms that are sensitive to the magnitude of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc33278-fee3-497f-ab8b-a8274b4a8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you have a dataset containing the ages and incomes of individuals:\n",
    "import numpy as np\n",
    "data = np.array([[25,50000],\n",
    "                [40,80000],\n",
    "                [30,60000],\n",
    "                [22,45000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f004ba4-6e9a-435d-9541-d1a60d9c5c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[   25 50000]\n",
      " [   40 80000]\n",
      " [   30 60000]\n",
      " [   22 45000]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.16666667 0.14285714]\n",
      " [1.         1.        ]\n",
      " [0.44444444 0.42857143]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# initialize the Minmaxscaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21305cd-c6b7-4e57-b430-a7ff4e36ced6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c393d-3004-4c8d-a642-a2437cd8d59d",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Vector Normalization, is a feature scaling method that involves scaling the values of each feature in a dataset to have a unit norm, which means that the length of the feature vector becomes 1. This technique is particularly useful when the direction of the data points is more important than their magnitude. It's commonly used in scenarios where you want to measure the similarity between data points based on their directions.\n",
    "\n",
    "The main difference between Unit Vector scaling and Min-Max scaling (normalization) is that Unit Vector scaling focuses on the direction of the data points, while Min-Max scaling aims to transform the values to a specific range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e5d9d1-b67a-433f-92b7-14937ba61534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[3 1]\n",
      " [4 2]\n",
      " [1 3]]\n",
      "\n",
      "Normalized Data:\n",
      "[[0.9486833  0.31622777]\n",
      " [0.89442719 0.4472136 ]\n",
      " [0.31622777 0.9486833 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "data = np.array([[3,1],\n",
    "                [4,2],\n",
    "                [1,3]])\n",
    "\n",
    "nor = Normalizer(norm='l2')\n",
    "\n",
    "nor_data = nor.transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(nor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eed3c9-26bc-489f-935f-d042eab19ef6",
   "metadata": {},
   "source": [
    "In this example, we use the Normalizer from the sklearn.preprocessing module. We set the norm parameter to 'l2' to apply L2 normalization, which scales each data point's feature vector to have a Euclidean norm (length) of 1.\n",
    "\n",
    "As shown in the output, the values of each feature are scaled such that the Euclidean norm of each row (feature vector) becomes 1. This ensures that each data point's direction in the feature space is preserved, which can be important for distance-based algorithms or when measuring the similarity between data points based on their directions.\n",
    "\n",
    "In summary, the Unit Vector technique (Vector Normalization) focuses on the direction of feature vectors, making it suitable for scenarios where the angle or direction between data points is more important than their magnitudes. On the other hand, Min-Max scaling aims to rescale feature values within a specific range, making them comparable in terms of magnitude.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aacc73-aeef-44f1-90e0-bad9a35d7940",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58caeb7e-2900-4e35-b99f-6f340f1579b0",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a dimensionality reduction technique used in statistics and machine learning to transform a high-dimensional dataset into a new coordinate system where the data's variance is maximized along the principal components (orthogonal axes). The main goal of PCA is to capture the most important patterns and variations in the data while reducing the number of dimensions.\n",
    "\n",
    "PCA works by identifying a set of orthogonal axes (principal components) that are aligned with the directions of maximum variance in the data. The first principal component corresponds to the axis with the highest variance, the second principal component is orthogonal to the first and has the second highest variance, and so on. By projecting the original data onto these principal components, you can effectively reduce the dimensionality of the data while preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c8b282-2f42-4689-a189-144e3741e71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "\n",
      "Reduced Data:\n",
      "[[ 4.24264069]\n",
      " [ 1.41421356]\n",
      " [-1.41421356]\n",
      " [-4.24264069]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = np.array([[2,3],\n",
    "                [4,5],\n",
    "                [6,7],\n",
    "                [8,9]]\n",
    "               )\n",
    "# initialize \n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "reduce_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nReduced Data:\")\n",
    "print(reduce_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe47bfda-3c42-499e-a9c7-e4c7c85ce76b",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with two features (2-dimensional). We apply PCA to reduce the dimensionality to 1 principal component. The PCA algorithm calculates the direction of maximum variance and projects the data onto this principal component.\n",
    "\n",
    "The output shows that the data has been transformed from a 2-dimensional space to a 1-dimensional space along the principal component axis. The values in the reduced data are the projections of the original data points onto the first principal component.\n",
    "\n",
    "PCA is widely used in various applications, such as image compression, feature extraction, and noise reduction. By reducing the dimensionality of the data while retaining as much information as possible, PCA can help improve the efficiency and effectiveness of machine learning algorithms while mitigating the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58a131-f385-415d-85b8-269bf6f10e53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea2fe9-32e8-45d2-8e39-d011a61073bc",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique that is often used for feature extraction, especially in scenarios where you have a high-dimensional dataset and want to reduce the dimensionality while preserving the most important information. Feature extraction involves transforming the original features into a new set of features that captures the essential information in the data while discarding less relevant information. PCA achieves this by finding the directions of maximum variance in the data and projecting the data onto these directions (principal components).\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA is a method for extracting a reduced set of features from the original data that still captures a significant portion of the variability in the data. In this sense, PCA serves as a form of feature extraction by creating a new representation of the data using linear combinations of the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf667bff-55b8-48d5-908e-82540f1b1cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[2 4 5]\n",
      " [3 5 6]\n",
      " [4 5 7]\n",
      " [5 6 8]]\n",
      "\n",
      "Extracted Features:\n",
      "[[-2.3439235  -0.07760566]\n",
      " [-0.64922922  0.28018104]\n",
      " [ 0.64922922 -0.28018104]\n",
      " [ 2.3439235   0.07760566]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = np.array([[2,4,5],\n",
    "                [3,5,6],\n",
    "                [4,5,7],\n",
    "                [5,6,8]])\n",
    "\n",
    "# initialize\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data using PCA for feature extraction\n",
    "\n",
    "extracted_features = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nExtracted Features:\")\n",
    "print(extracted_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf60758-b22d-4d69-b8cb-4627dc98701e",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with three features (3-dimensional). We apply PCA to extract 2 principal components. The extracted features are the transformed data points projected onto these two principal components.\n",
    "\n",
    "As shown in the output, the extracted features are a linear combination of the original features. These new features represent directions in the original feature space that capture the most variance in the data. By reducing the dimensionality from 3 to 2 while retaining meaningful information, PCA has performed feature extraction.\n",
    "\n",
    "Feature extraction using PCA is particularly useful in reducing noise, improving model efficiency, and dealing with the curse of dimensionality in machine learning tasks. It helps in creating a more compact and informative representation of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93764ce-d3c2-41c7-a6ba-2f302efa30f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0638d1-4ea4-4016-8a42-c36d5634c142",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the numerical features such as price, rating, and delivery time. Min-Max scaling will transform these features to a common range (typically between 0 and 1) so that they are comparable and won't bias the recommendation algorithm towards features with larger magnitudes. Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "Understand the Data:\n",
    "Begin by understanding the dataset and the features you have. Identify which features are numerical and need to be scaled.\n",
    "\n",
    "Import Libraries:\n",
    "Import the necessary libraries for data preprocessing. In this case, you can use libraries like NumPy or scikit-learn in Python.\n",
    "\n",
    "Extract Numerical Features:\n",
    "Create a subset of the dataset that includes only the numerical features you want to scale (e.g., price, rating, delivery time).\n",
    "\n",
    "Min-Max Scaling:\n",
    "Apply Min-Max scaling to each feature. For each feature, subtract the minimum value of that feature and then divide by the range (maximum value - minimum value). This will ensure that the scaled values fall within the range [0, 1].\n",
    "\n",
    "Transform the Data:\n",
    "Replace the original numerical feature values with their scaled counterparts in the dataset.\n",
    "\n",
    "Use the Scaled Data for Recommendation:\n",
    "Utilize the scaled dataset as input for building your recommendation system. The scaled features will ensure that no particular feature dominates the recommendation process due to its scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abad162b-5e24-4b37-8bf6-745a28e0cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[10.   4.5 30. ]\n",
      " [15.   3.8 45. ]\n",
      " [ 8.   4.2 25. ]\n",
      " [20.   4.9 60. ]]\n",
      "\n",
      "Scaled Data:\n",
      "[[0.16666667 0.63636364 0.14285714]\n",
      " [0.58333333 0.         0.57142857]\n",
      " [0.         0.36363636 0.        ]\n",
      " [1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# price , rating , delivery_time\n",
    "data = np.array([[10, 4.5, 30],\n",
    "                 [15, 3.8, 45],\n",
    "                 [8, 4.2, 25],\n",
    "                 [20, 4.9, 60]])\n",
    "\n",
    "# Initialize\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075352b2-8df0-4993-b545-db4fc99d06ed",
   "metadata": {},
   "source": [
    "In this example, each feature has been scaled using Min-Max scaling, resulting in scaled values between 0 and 1 for each feature. These scaled features can then be used as input for building your recommendation system, ensuring that the recommendation process is not biased by the original scale of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4322d-0177-4c2f-8d04-ec849d50c82e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c2146-2f80-48f1-9294-af27e2b3a28d",
   "metadata": {},
   "source": [
    "Using PCA to reduce the dimensionality of a dataset when building a stock price prediction model can help mitigate the curse of dimensionality and improve the efficiency of the model while retaining important patterns in the data. Here's how you would use PCA for dimensionality reduction in the context of predicting stock prices:\n",
    "\n",
    "Understand the Data:\n",
    "Begin by understanding the dataset and its features. Identify which features are relevant for predicting stock prices, including company financial data and market trends.\n",
    "\n",
    "Data Preprocessing:\n",
    "Preprocess the data by handling missing values, normalizing or standardizing the features if necessary, and ensuring that the data is clean and ready for analysis.\n",
    "\n",
    "Extract Relevant Features:\n",
    "Create a subset of the dataset that includes the relevant features for predicting stock prices. This subset will be the input for the PCA process.\n",
    "\n",
    "Standardization:\n",
    "Standardize the features in the subset so that they all have mean 0 and standard deviation 1. This step is important for PCA because it ensures that features with larger variances don't dominate the PCA process.\n",
    "\n",
    "PCA Application:\n",
    "Apply PCA to the standardized feature subset. The goal of PCA is to identify the principal components that capture the most significant variance in the data.\n",
    "\n",
    "Determine Number of Principal Components:\n",
    "Decide on the number of principal components to retain. This can be based on the cumulative explained variance, where you aim to retain a certain percentage of the total variance in the data.\n",
    "\n",
    "Perform Dimensionality Reduction:\n",
    "Transform the standardized features into a new set of features using the selected number of principal components. These new features are linear combinations of the original features and represent directions of maximum variance in the data.\n",
    "\n",
    "Use Reduced Dimension Data for Modeling:\n",
    "Utilize the reduced dimension dataset as input for building your stock price prediction model. The reduced dataset contains features that capture the most important information while having a lower dimensionality than the original dataset.\n",
    "\n",
    "Model Building and Evaluation:\n",
    "Build your prediction model using the reduced dimension dataset. Use appropriate machine learning algorithms, such as regression or time series models, and evaluate the model's performance using appropriate metrics.\n",
    "\n",
    "Interpretation:\n",
    "The principal components can be interpreted to understand which combinations of original features contribute most to the variance in the data. This can provide insights into the factors influencing stock price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5651b61-fe49-4b8b-ab91-994cc7041086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (100, 20)\n",
      "Reduced Dimension Data Shape: (100, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = np.random.rand(100,20) # 100 samples and 20 features\n",
    "\n",
    "# Standardlize the data\n",
    "\n",
    "mean = np.mean(data,axis=0)\n",
    "std = np.std(data,axis=0)\n",
    "standardized_data = (data - mean) / std\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "reduced_data = pca.fit_transform(standardized_data)\n",
    "\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"Reduced Dimension Data Shape:\", reduced_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b531922-a8ef-4ff8-80bc-c0edd1216fa3",
   "metadata": {},
   "source": [
    "In this example, the dataset contains 100 samples with 20 features. After applying PCA with 10 principal components, the dimensionality is reduced to 10 features. These 10 principal components capture the most significant patterns in the data while reducing the dimensionality for the stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d9871-c0f4-4443-bc25-b3ddc7ac7be3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af3803d4-6d8a-47d4-96ef-8a0d22a2bab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [15]\n",
      " [20]]\n",
      "\n",
      "Scaled Data:\n",
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "original_data = np.array([1,5,10,15,20]).reshape(-1,1)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaled_data = scaler.fit_transform(original_data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(original_data)\n",
    "\n",
    "print(\"\\nScaled Data:\")\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b32f5f-b465-4e11-b724-af23b24a613d",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bc07398-128b-43ea-b355-96f76ae7c706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (5, 5)\n",
      "Reduced Dimension Data Shape: (5, 1)\n",
      "Number of Components to Retain: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Heights , Weight , Age , Gender , Blood pressure\n",
    "data = np.array([[160, 60, 30, 0, 120],\n",
    "                 [170, 65, 25, 1, 130],\n",
    "                 [155, 50, 40, 0, 110],\n",
    "                 [175, 70, 28, 1, 140],\n",
    "                 [165, 55, 35, 0, 125]])\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "cumulative_explained_varience = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "desired_varience_threshold = 0.95\n",
    "num_components_to_retain = np.argmax(cumulative_explained_varience >= desired_varience_threshold)\n",
    "\n",
    "reduced_data = pca.transform(data)[:, :num_components_to_retain]\n",
    "\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"Reduced Dimension Data Shape:\", reduced_data.shape)\n",
    "print(\"Number of Components to Retain:\", num_components_to_retain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3351a3-d69e-4ae0-8197-895815e042cc",
   "metadata": {},
   "source": [
    "\n",
    "In this code example, I'll provide you with a Python implementation of how to perform feature extraction using PCA on the given dataset containing features [height, weight, age, gender, blood pressure]. I'll also explain how to choose the number of principal components to retain based on the explained variance.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample dataset with features [height, weight, age, gender, blood pressure]\n",
    "data = np.array([[160, 60, 30, 0, 120],\n",
    "                 [170, 65, 25, 1, 130],\n",
    "                 [155, 50, 40, 0, 110],\n",
    "                 [175, 70, 28, 1, 140],\n",
    "                 [165, 55, 35, 0, 125]])\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the data\n",
    "pca.fit(data)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Choose the number of components to retain based on desired explained variance threshold\n",
    "desired_variance_threshold = 0.95\n",
    "num_components_to_retain = np.argmax(cumulative_explained_variance >= desired_variance_threshold) + 1\n",
    "\n",
    "# Transform the data using the chosen number of components\n",
    "reduced_data = pca.transform(data)[:, :num_components_to_retain]\n",
    "\n",
    "print(\"Original Data Shape:\", data.shape)\n",
    "print(\"Reduced Dimension Data Shape:\", reduced_data.shape)\n",
    "print(\"Number of Components to Retain:\", num_components_to_retain)\n",
    "Explanation of the code:\n",
    "\n",
    "We use a sample dataset with features [height, weight, age, gender, blood pressure].\n",
    "Initialize PCA without specifying the number of components. This means PCA will retain all components.\n",
    "Fit PCA on the data.\n",
    "Calculate the cumulative explained variance by summing up the explained variance ratio for each component.\n",
    "Choose the number of components to retain based on the desired explained variance threshold. In this example, we choose a threshold of 0.95 (95% variance).\n",
    "Transform the data using the chosen number of components to obtain the reduced-dimension dataset.\n",
    "Print the shapes of the original and reduced-dimension data and the number of components to retain.\n",
    "The code will output the number of principal components to retain based on the desired explained variance threshold. You can adjust the desired_variance_threshold variable to control the amount of variance you want to retain in your reduced-dimension dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5a33e-da50-438f-bf82-a39f84d2232d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
